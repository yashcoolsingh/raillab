{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started with Gym\n",
    "==============\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms that makes no assumptions about the structure of your agent.\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments\n",
    "--------------\n",
    "\n",
    "Here’s a bare minimum example of getting something running. This will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step. You should see a window pop up rendering the classic cart-pole problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MsPacman-v0')\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to see some other environments in action, try replacing MsPacman-v0 above with something like MountainCar-v0, or Hopper-v1 (requires the MuJoCo dependencies). Environments all descend from the Env base class.\n",
    "\n",
    "Note that if you’re missing any dependencies, you should get a helpful error message telling you what you’re missing. Installing a missing dependency is generally pretty simple. You’ll also need a MuJoCo license for Hopper-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "--------------\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it’d probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment’s step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "1. `observation` (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "2. `reward` (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "3. `done` (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "4. `info` (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "The process gets started by calling `reset()`, which returns an initial observation. So a more proper way of writing the previous code would be to respect the `done` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "for i_episode in range(2):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spaces\n",
    "--------\n",
    "\n",
    "In the examples above, we’ve been sampling random actions from the environment’s action space. But what actually are those actions? Every environment comes with an `action_space` and an `observation_space`. These attributes are of type `Space`, and they describe the format of valid actions and observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Discrete` space allows a fixed range of non-negative numbers, so in this case valid actions are either 0 to 8. The `Box` space represents an n-dimensional box, so valid observations will be an RGB image of size 210x160. We can also check the Box’s bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.high[:, :, 0])\n",
    "print(env.observation_space.low[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introspection can be helpful to write generic code that works for many different environments. `Box` and `Discrete` are the most common `Spaces`. You can sample from a `Space` or check that something belongs to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "space = spaces.Discrete(8) # Set with 8 elements {0, 1, 2, ..., 7}\n",
    "x = space.sample()\n",
    "print(x)\n",
    "print(space.contains(x))\n",
    "print(space.n == 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Environments\n",
    "----------------------------\n",
    "\n",
    "Gym comes with a diverse suite of environments that range from easy to difficult and involve many different kinds of data. View the full list of environments to get the birds-eye view.\n",
    "\n",
    "1. Classic control and toy text: complete small-scale tasks, mostly from the RL literature. They’re here to get you started.\n",
    "2. Algorithmic: perform computations such as adding multi-digit numbers and reversing sequences. One might object that these tasks are easy for a computer. The challenge is to learn these algorithms purely from examples. These tasks have the nice property that it’s easy to vary the difficulty by varying the sequence length.\n",
    "3. Atari: play classic Atari games. We’ve integrated the Arcade Learning Environment (which has had a big impact on reinforcement learning research) in an easy-to-install form.\n",
    "4. 2D and 3D robots: control a robot in simulation. These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrappers\n",
    "---------\n",
    "\n",
    "Wrappers are used to transform an environment in a modular way. For example, here is a wrapper to downsample the RGB observations of an environment and convert them to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wrap MsPacman-v0 using `WarpFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "print(env.observation_space.shape)\n",
    "env = WarpFrame(env)\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Complete the missing lines to create your own gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5 # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max])\n",
    "        low = -high\n",
    "\n",
    "        # TODO: Define the action space. There are 2 possible actions: \n",
    "        # apply force from the left and apply force from the right\n",
    "        self.action_space = ...\n",
    "        \n",
    "        # TODO: Define the observation space. The range of values is defined above using low and high.\n",
    "        self.observation_space = ...\n",
    "\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: check that the action is valid i.e. action is contained in the action_space\n",
    "        \n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        \n",
    "        # TODO: use action to decide force to apply to the cart (i.e. whether to use -force_mag or force_mag)\n",
    "        force = ...\n",
    "        \n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        \n",
    "        # Use the equations of motion to update the state variables\n",
    "        x = ...\n",
    "        x_dot = ...\n",
    "        theta = ...\n",
    "        theta_dot = ...\n",
    "        \n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "\n",
    "        # TODO: if the pole hasn't fallen over the reward is 1 else set it to 0.\n",
    "        reward = ...\n",
    "        \n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: Reset the environment by sampling the state uniformly in the range of -0.05 to -0.05 \n",
    "        self.state = ...\n",
    "\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer: self.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Typically you'd register the environment and use gym.make but we're skipping this for convinience.\n",
    "env = CartPoleEnv()\n",
    "obs = env.reset()\n",
    "for _ in range(20):\n",
    "    print(obs)\n",
    "    env.render()\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
